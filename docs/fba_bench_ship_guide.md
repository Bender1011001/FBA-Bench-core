# FBA-Bench Core: Production Ship-Readiness Guide

**Version**: 1.0  
**Target Audience**: Development Team  
**Objective**: Transform FBA-Bench-core from staging to production-ready

---

## Table of Contents

1. [Critical Path to Ship (P0)](#critical-path-to-ship-p0)
2. [Production Hardening (P1)](#production-hardening-p1)
3. [Developer Experience (P2)](#developer-experience-p2)
4. [Complete Implementation Checklist](#complete-implementation-checklist)
5. [Technical Specifications](#technical-specifications)
6. [Security & Compliance](#security--compliance)
7. [Testing Strategy](#testing-strategy)
8. [Deployment & Release Process](#deployment--release-process)

---
## Ship-Readiness Execution Status

- P0 Critical Path to Ship: Completed (with one blocked subtrack; see below)
- P1 Production Hardening: Completed
- P2 Developer Experience: Completed

Blocked item (P0):
- Awaiting real ARXIV_ID. Once provided, replace token ARXIV_ID across:
  - [CITATION.cff](CITATION.cff)
  - [README.md](README.md)
  - [site/research.html](site/research.html)
  - [docs/paper/README.md](docs/paper/README.md)

Repository-wide deliverables added during execution:
- CI/Validation: [.github/workflows/ci.yml](.github/workflows/ci.yml), [scripts/validate_all.py](scripts/validate_all.py), [scripts/validate_scenarios.py](scripts/validate_scenarios.py), [schemas/leaderboard_schema.json](schemas/leaderboard_schema.json), [schemas/scenario_schema.json](schemas/scenario_schema.json)
- Secrets and hygiene: [.env.example](.env.example), [.pre-commit-config.yaml](.pre-commit-config.yaml), [.secrets.baseline](.secrets.baseline)
- Site/analytics: [site/config.js](site/config.js), [scripts/inject-config.sh](scripts/inject-config.sh)
- Licensing: [LICENSE](LICENSE), [scripts/add-license-header.py](scripts/add-license-header.py), [pyproject.toml](pyproject.toml)
- Assets: [scripts/build_assets.py](scripts/build_assets.py)
- Developer environment: [Dockerfile](Dockerfile), [docker-compose.yml](docker-compose.yml), [.devcontainer/devcontainer.json](.devcontainer/devcontainer.json), [docs/quickstart.md](docs/quickstart.md)
---
## Critical Path to Ship (P0)

#### Status and Outputs

Status: Completed. One subtrack remains blocked: provide ARXIV_ID and run the documented replacement.

Key outputs:
- Secrets and hygiene:
  - [.env.example](.env.example), [.pre-commit-config.yaml](.pre-commit-config.yaml), [.secrets.baseline](.secrets.baseline)
- Analytics config and injection:
  - [site/config.js](site/config.js), [scripts/inject-config.sh](scripts/inject-config.sh)
- Licensing and metadata:
  - [LICENSE](LICENSE), [pyproject.toml](pyproject.toml), [scripts/add-license-header.py](scripts/add-license-header.py)
- Documentation and site updates:
  - [README.md](README.md), [CITATION.cff](CITATION.cff), [docs/paper/README.md](docs/paper/README.md), [site/index.html](site/index.html), [site/press.html](site/press.html), [site/research.html](site/research.html)
- CI/Validation:
  - [.github/workflows/ci.yml](.github/workflows/ci.yml), [schemas/leaderboard_schema.json](schemas/leaderboard_schema.json), [schemas/scenario_schema.json](schemas/scenario_schema.json), [scripts/validate_all.py](scripts/validate_all.py), [scripts/validate_scenarios.py](scripts/validate_scenarios.py)

Note: ARXIV_ID is parameterized and documented for a one-line repository-wide replacement once the identifier is issued.

**Timeline**: 2-3 weeks
**Blockers**: These MUST be completed before any public release

### 1. Replace All Temporary Content

#### Task 1.1: Update Contact Information
```bash
# Files to update:
- README.md
- site/index.html
- site/press.html
- site/research.html
- docs/paper/README.md
```

**Find and replace:**
```bash
# Search for common staging markers in prose and configs
grep -r "example.com" .
# Also inspect for temporary markers and WIP tags (avoid committing them)
```
_Implemented via repository-wide scrubbing and CI checks; remaining pending item is ARXIV_ID substitution (see Next Actions)._

**Action items:**
- [ ] Set up official email: `contact@fba-bench.ai` or similar
- [ ] Create press contact: `press@fba-bench.ai`
- [ ] Register domain if using custom domain
- [ ] Update all email references

#### Task 1.2: Obtain arXiv Identifier
```bibtex
# Current status:
# arXiv identifier pending (use ARXIV_ID token for substitution), example: arXiv:2501.12345

# Required action:
# 1. Submit paper to arXiv
# 2. Obtain official identifier (e.g., arXiv:2501.12345)
# 3. Update CITATION.cff and all BibTeX examples
```
_Provide real ARXIV_ID and run documented replacement; single-source in [CITATION.cff](CITATION.cff) and [docs/paper/README.md](docs/paper/README.md)._

**Files to update:**
- `CITATION.cff`
- `README.md` (BibTeX section)
- `site/research.html`
- `docs/paper/README.md`

#### Task 1.3: Complete Author Information
```yaml
# CITATION.cff needs real authors:
authors:
  - family-names: "Doe"
    given-names: "John"
    orcid: "https://orcid.org/0000-0000-0000-0000"
  # Add all actual contributors
```

---

### 2. Implement Secrets Management

#### Task 2.1: Create Environment Configuration

**Create `.env.example`:**
```bash
# Analytics Configuration
ANALYTICS_ENABLED=false
ANALYTICS_TRACKING_ID=
ANALYTICS_ENDPOINT=
ANALYTICS_RESPECT_DNT=true

# API Keys (if needed)
API_KEY=
SECRET_KEY=

# Environment
ENVIRONMENT=development
```
_Implemented in repo at [.env.example](.env.example)._

**Create `.env` (gitignored):**
```bash
cp .env.example .env
# Edit .env with actual values (never commit)
```

#### Task 2.2: Update .gitignore
```gitignore
# Environment files
.env
.env.local
.env.production
*.env.backup

# Secrets
secrets/
*.key
*.pem
credentials.json

# Build artifacts
dist/
build/
*.pyc
__pycache__/

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Testing
.pytest_cache/
.coverage
htmlcov/

# Virtual environments
.venv/
venv/
env/
```

#### Task 2.3: Install Pre-commit Hooks

**Install detect-secrets:**
```bash
pip install pre-commit detect-secrets
```

**Create `.pre-commit-config.yaml`:**
```yaml
repos:
  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.4.0
    hooks:
      - id: detect-secrets
        args: ['--baseline', '.secrets.baseline']
        exclude: package.lock.json
  
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-json
      - id: check-merge-conflict
      - id: detect-private-key
  
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
```
_Implemented in repo at [.pre-commit-config.yaml](.pre-commit-config.yaml); secrets baseline generated locally (`.secrets.baseline`) and may be untracked._

**Initialize and install:**
```bash
detect-secrets scan > .secrets.baseline
pre-commit install
pre-commit run --all-files  # Test
```

#### Task 2.4: Refactor Analytics Configuration

**Update `site/config.js`:**
Implemented at [site/config.js](site/config.js)
```javascript
// Load from environment or build-time injection
window.ANALYTICS_CONFIG = {
  enabled: false, // Override via build process
  provider: "unset",
  trackingId: "", // Injected at build time
  respectDNT: true,
  sampleRate: 1.0,
  endpoint: "" // Injected at build time
};

// Never commit real values to this file
// Use build scripts to inject production values
```
_Implemented in repo at [site/config.js](site/config.js). Values injected via [scripts/inject-config.sh](scripts/inject-config.sh)._

**Create build script** (`scripts/inject-config.sh`):
Implemented at [scripts/inject-config.sh](scripts/inject-config.sh)
```bash
#!/bin/bash
set -e

ENV=${1:-development}

if [ "$ENV" = "production" ]; then
  # Inject production values from environment
  sed -i.bak "s/enabled: false/enabled: ${ANALYTICS_ENABLED:-false}/" site/config.js
  sed -i.bak "s/trackingId: \"\"/trackingId: \"${ANALYTICS_TRACKING_ID}\"/" site/config.js
  sed -i.bak "s/endpoint: \"\"/endpoint: \"${ANALYTICS_ENDPOINT}\"/" site/config.js
  echo "âœ… Injected production analytics config"
else
  echo "âœ… Using development config (no analytics)"
fi
```
_Implemented in repo at [scripts/inject-config.sh](scripts/inject-config.sh)._

---

### 3. Choose and Specify License

#### Task 3.1: Select License

**Recommended options based on project goals:**

| License | Use Case | Key Terms |
|---------|----------|-----------|
| **MIT** | Maximum permissiveness, encourage adoption | Can be used commercially, must include license |
| **Apache 2.0** | Patent protection, permissive | Patent grant, trademark protection |
| **GPL v3** | Require derivative works to be open-source | Copyleft, derivative works must be GPL |
| **AGPL v3** | GPL + network use triggers | Covers SaaS/API usage |
| **Business Source License** | Open but not free for commercial use | Converts to open source after time period |

**Decision framework:**
- Want wide adoption? â†’ **MIT** or **Apache 2.0**
- Want to prevent proprietary forks? â†’ **GPL v3** or **AGPL v3**
- Want eventual open source but monetize initially? â†’ **Business Source License**

#### Task 3.2: Implement License

Implemented choice: MIT; see [LICENSE](LICENSE) and [pyproject.toml](pyproject.toml) aligned.

**Create proper LICENSE file:**
```bash
# For MIT:
curl -o LICENSE https://raw.githubusercontent.com/licenses/license-templates/master/templates/mit.txt

# Edit with:
# - Current year
# - Copyright holder name
# - Organization name
```

**Add license headers to all source files:**

**Python files** (`scripts/add-license-header.py`):
```python
#!/usr/bin/env python3
"""Add license headers to Python files."""

HEADER = '''"""
Copyright (c) 2025 [Your Organization]
Licensed under the [License Name] License.
See LICENSE file in the project root for full license information.
"""
'''

import os
import glob

for filepath in glob.glob('**/*.py', recursive=True):
    if filepath.startswith('.venv') or filepath.startswith('build'):
        continue
    
    with open(filepath, 'r+') as f:
        content = f.read()
        if 'Copyright' not in content:
            f.seek(0, 0)
            f.write(HEADER + '\n' + content)
            print(f"âœ… Added header to {filepath}")
```
_Implemented in repo at [scripts/add-license-header.py](scripts/add-license-header.py)._

**Update all documentation:**
```markdown
## License

This project is licensed under the [LICENSE NAME] - see the [LICENSE](LICENSE) file for details.
```

---

### 4. Implement CI/CD Pipeline

#### Task 4.1: Create GitHub Actions Workflows

**`.github/workflows/ci.yml`:**
Implemented at [.github/workflows/ci.yml](.github/workflows/ci.yml)
```yaml
name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install ruff black mypy
          pip install -e .
      
      - name: Run ruff
        run: ruff check .
      
      - name: Run black
        run: black --check .
      
      - name: Run mypy
        run: mypy src/
        continue-on-error: true  # Remove after fixing type issues

  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install pytest pytest-cov
          pip install -e .
      
      - name: Run tests
        run: pytest --cov=src --cov-report=xml --cov-report=term
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: false

  validate-golden-masters:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Validate JSON files
        run: |
          python - <<'PY'
          import os, json, glob, sys
          gm_dir = 'golden_masters'
          if os.path.isdir(gm_dir):
              fail = False
              for p in glob.glob(os.path.join(gm_dir, '**', '*.json'), recursive=True):
                  try:
                      json.load(open(p, 'r', encoding='utf-8'))
                      print(f"âœ… {p}")
                  except Exception as e:
                      print(f"âŒ {p}: {e}")
                      fail = True
              sys.exit(1 if fail else 0)
          else:
              print("âš ï¸  golden_masters/ not found")
              sys.exit(0)
          PY
      
      - name: Validate scenario YAML
        run: |
          pip install pyyaml jsonschema
          python scripts/validate_scenarios.py

  validate-site:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Validate HTML
        uses: Cyb3r-Jak3/html5validator-action@v7.2.0
        with:
          root: site/
          extra: --ignore-re "Element .* not allowed as child"
      
      - name: Validate leaderboard JSON
        run: |
          python - <<'PY'
          import json, sys
          from jsonschema import validate, ValidationError
          
          schema = {
              "type": "array",
              "items": {
                  "type": "object",
                  "required": ["team", "model", "score", "date"],
                  "properties": {
                      "team": {"type": "string"},
                      "model": {"type": "string"},
                      "score": {"type": "number", "minimum": 0, "maximum": 100},
                      "date": {"type": "string", "pattern": "^\d{4}-\d{2}-\d{2}$"},
                      "notes": {"type": "string"}
                  },
      "additionalProperties": true
    },
    "evaluation": {
      "type": "object",
      "required": ["metrics"],
      "properties": {
        "metrics": {
          "type": "array",
          "minItems": 1,
          "items": {
            "type": "string",
            "enum": ["trust", "operations", "finance", "marketing", "stress"]
          }
        },
        "thresholds": {
          "type": "object",
          "properties": {
            "pass": {
              "type": "number",
              "minimum": 0,
              "maximum": 100
            },
            "excellent": {
              "type": "number",
              "minimum": 0,
              "maximum": 100
            }
          }
        }
      }
    },
    "metadata": {
      "type": "object",
      "properties": {
        "version": {"type": "string"},
        "created": {"type": "string", "format": "date-time"},
        "updated": {"type": "string", "format": "date-time"},
        "author": {"type": "string"}
      }
    }
  }
}
```
_Implemented in repo at [.github/workflows/ci.yml](.github/workflows/ci.yml)._

**`schemas/leaderboard_schema.json`:**
Implemented at [schemas/leaderboard_schema.json](schemas/leaderboard_schema.json) and [schemas/scenario_schema.json](schemas/scenario_schema.json)
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "FBA-Bench Leaderboard",
  "type": "array",
  "items": {
    "type": "object",
    "required": ["team", "model", "score", "date"],
    "properties": {
      "team": {
        "type": "string",
        "minLength": 1,
        "maxLength": 100
      },
      "model": {
        "type": "string",
        "minLength": 1,
        "maxLength": 100
      },
      "score": {
        "type": "number",
        "minimum": 0,
        "maximum": 100
      },
      "date": {
        "type": "string",
        "pattern": "^\\d{4}-\\d{2}-\\d{2}$",
        "description": "Submission date in YYYY-MM-DD format"
      },
      "notes": {
        "type": "string",
        "maxLength": 500
      },
      "url": {
        "type": "string",
        "format": "uri",
        "description": "Optional link to model or paper"
      },
      "tier": {
        "type": "integer",
        "minimum": 0,
        "maximum": 2,
        "description": "Highest tier completed"
      }
    },
    "additionalProperties": false
  }
}
```
_Implemented schemas at [schemas/leaderboard_schema.json](schemas/leaderboard_schema.json), [schemas/scenario_schema.json](schemas/scenario_schema.json)._

#### Task 8.2: Enhanced Validation Script

**`scripts/validate_all.py`:**
```python
#!/usr/bin/env python3
"""Comprehensive validation of all data files."""

import json
import sys
from pathlib import Path
from typing import List, Tuple

import yaml
from jsonschema import validate, ValidationError, Draft7Validator

def load_schema(schema_name: str) -> dict:
    """Load a JSON schema file."""
    schema_path = Path("schemas") / f"{schema_name}.json"
    with open(schema_path) as f:
        return json.load(f)

def validate_scenarios() -> List[Tuple[str, bool, str]]:
    """Validate all scenario YAML files."""
    results = []
    schema = load_schema("scenario_schema")
    validator = Draft7Validator(schema)
    
    scenario_dir = Path("src/fba_bench_core/scenarios")
    if not scenario_dir.exists():
        return [("scenarios", False, "Directory not found")]
    
    for yaml_file in scenario_dir.glob("*.yaml"):
        try:
            with open(yaml_file) as f:
                data = yaml.safe_load(f)
            
            # Validate against schema
            errors = list(validator.iter_errors(data))
            if errors:
                error_msg = "; ".join([e.message for e in errors[:3]])
                results.append((str(yaml_file), False, error_msg))
            else:
                results.append((str(yaml_file), True, "Valid"))
        
        except Exception as e:
            results.append((str(yaml_file), False, str(e)))
    
    return results

def validate_leaderboard() -> List[Tuple[str, bool, str]]:
    """Validate leaderboard JSON."""
    results = []
    schema = load_schema("leaderboard_schema")
    
    leaderboard_path = Path("site/data/leaderboard.json")
    
    if not leaderboard_path.exists():
        return [("leaderboard.json", False, "File not found")]
    
    try:
        with open(leaderboard_path) as f:
            data = json.load(f)
        
        validate(instance=data, schema=schema)
        
        # Additional validations
        if not data:
            results.append(("leaderboard.json", True, "Valid (empty)"))
        else:
            # Check for duplicates
            entries = [(e["team"], e["model"]) for e in data]
            if len(entries) != len(set(entries)):
                results.append(("leaderboard.json", False, "Duplicate entries found"))
            else:
                results.append(("leaderboard.json", True, f"Valid ({len(data)} entries)"))
    
    except ValidationError as e:
        results.append(("leaderboard.json", False, e.message))
    except Exception as e:
        results.append(("leaderboard.json", False, str(e)))
    
    return results

def validate_golden_masters() -> List[Tuple[str, bool, str]]:
    """Validate golden master JSON files."""
    results = []
    gm_dir = Path("golden_masters")
    
    if not gm_dir.exists():
        return [("golden_masters", False, "Directory not found")]
    
    json_files = list(gm_dir.rglob("*.json"))
    
    if not json_files:
        return [("golden_masters", True, "No JSON files (OK)")]
    
    for json_file in json_files:
        try:
            with open(json_file) as f:
                json.load(f)
            results.append((str(json_file), True, "Valid JSON"))
        except Exception as e:
            results.append((str(json_file), False, str(e)))
    
    return results

def print_results(category: str, results: List[Tuple[str, bool, str]]):
    """Print validation results for a category."""
    print(f"\n{'='*80}")
    print(f"  {category}")
    print(f"{'='*80}")
    
    for filepath, passed, message in results:
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {filepath}")
        if not passed or message != "Valid":
            print(f"   â””â”€ {message}")
    
    passed_count = sum(1 for _, p, _ in results if p)
    total_count = len(results)
    print(f"\nğŸ“Š {passed_count}/{total_count} passed")

def main():
    print("ğŸ” Running comprehensive validation...\n")
    
    all_passed = True
    
    # Validate scenarios
    scenario_results = validate_scenarios()
    print_results("SCENARIOS", scenario_results)
    all_passed &= all(p for _, p, _ in scenario_results)
    
    # Validate leaderboard
    leaderboard_results = validate_leaderboard()
    print_results("LEADERBOARD", leaderboard_results)
    all_passed &= all(p for _, p, _ in leaderboard_results)
    
    # Validate golden masters
    gm_results = validate_golden_masters()
    print_results("GOLDEN MASTERS", gm_results)
    all_passed &= all(p for _, p, _ in gm_results)
    
    print(f"\n{'='*80}")
    if all_passed:
        print("âœ¨ All validations passed!")
        return 0
    else:
        print("âŒ Some validations failed")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```
_Implemented in repo at [scripts/validate_all.py](scripts/validate_all.py)._

---

### 9. Documentation Improvements

#### Task 9.1: Create Architecture Documentation

**`docs/architecture.md`:**
```markdown
# FBA-Bench Core Architecture

## Overview

FBA-Bench Core is designed as a modular, extensible benchmark suite for evaluating AI agents in financial business automation contexts.

## System Components

### 1. Scenario System

Scenarios are defined in YAML format with three complexity tiers:

- **Tier 0 (Baseline)**: Simple scenarios with minimal constraints
- **Tier 1 (Moderate)**: Realistic scenarios with operational constraints
- **Tier 2 (Advanced)**: Complex multi-agent scenarios with tight evaluation

**Scenario Structure:**
```yaml
id: example_scenario
name: "Example Scenario"
tier: 1
description: "Scenario description"
inputs:
  initial_budget: 10000
  time_limit: 3600
constraints:
  max_api_calls: 100
  allowed_actions: ["read", "write", "analyze"]
evaluation:
  metrics: ["trust", "operations", "finance"]
  thresholds:
    pass: 70.0
    excellent: 90.0
```

### 2. Metrics System

Metrics are organized by domain:

- **Trust Metrics**: Data privacy, consent management, security
- **Operations Metrics**: Efficiency, error rate, resource utilization
- **Finance Metrics**: Cost optimization, ROI, budget adherence
- **Marketing Metrics**: Campaign effectiveness, audience targeting
- **Stress Metrics**: Performance under load, error recovery

**Metric Interface:**
```python
from abc import ABC, abstractmethod
from typing import Dict, Any

class BaseMetric(ABC):
    @abstractmethod
    def calculate(self, agent_output: Dict[str, Any]) -> float:
        """Calculate metric score (0-100)."""
        pass
    
    @abstractmethod
    def get_details(self) -> Dict[str, Any]:
        """Get detailed breakdown of metric calculation."""
        pass
```

### 3. Agent Interface

Agents implement a standard interface for consistency:

```python
class BaseAgent(ABC):
    @abstractmethod
    def execute(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the scenario and return results."""
        pass
    
    @abstractmethod
    def reset(self) -> None:
        """Reset agent state between runs."""
        pass
```

### 4. Golden Masters

Golden masters are reference outputs that serve as regression detection points:

- Stored as JSON in `golden_masters/{tier}/`
- Validated in CI pipeline
- Updated deliberately when benchmark evolves

**Golden Master Format:**
```json
{
  "scenario_id": "example_scenario",
  "version": "1.0.0",
  "expected_output": {
    "actions_taken": 42,
    "resources_used": 850,
    "completion_time": 3200
  },
  "metric_scores": {
    "trust": 85.0,
    "operations": 92.0,
    "finance": 78.0
  }
}
```

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scenario   â”‚
â”‚    YAML     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agent     â”‚â”€â”€â”€â”€â–¶â”‚   Metrics    â”‚
â”‚ Execution   â”‚     â”‚  Evaluation  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Results   â”‚     â”‚    Scores    â”‚
â”‚   (JSON)    â”‚     â”‚   (0-100)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Extension Points

### Adding New Scenarios

1. Create YAML file in `src/fba_bench_core/scenarios/`
2. Validate against schema: `python scripts/validate_scenarios.py`
3. Generate golden master
4. Add tests

### Adding New Metrics

1. Implement `BaseMetric` interface
2. Add to `src/fba_bench_core/metrics/`
3. Register in metric suite
4. Update documentation

### Creating Baseline Agents

1. Implement `BaseAgent` interface
2. Add to `src/fba_bench_core/baseline_bots/`
3. Benchmark against existing agents
4. Document behavior and performance

## Performance Considerations

- Scenarios should complete in < 5 minutes on standard hardware
- Metrics should calculate in < 1 second
- Golden masters should be < 1 MB per scenario
- Full benchmark suite should run in < 30 minutes

## Version Compatibility

- Scenarios are versioned with semantic versioning
- Breaking changes require major version bump
- Golden masters track scenario version
- Backward compatibility maintained for minor versions
```

#### Task 9.2: Create Contributing Guide

**`CONTRIBUTING.md`:**
```markdown
# Contributing to FBA-Bench Core

Thank you for your interest in contributing! This guide will help you get started.

## Code of Conduct

Be respectful, inclusive, and professional. We welcome contributions from everyone.

## Getting Started

### 1. Fork and Clone

```bash
# Fork on GitHub, then:
git clone https://github.com/YOUR_USERNAME/FBA-Bench-core.git
cd FBA-Bench-core
```

### 2. Set Up Development Environment

```bash
# Create virtual environment
python3.9 -m venv .venv
source .venv/bin/activate  # or .\.venv\Scripts\Activate.ps1 on Windows

# Install with dev dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### 3. Create a Branch

```bash
git checkout -b feature/your-feature-name
# or
git checkout -b fix/issue-number-description
```

## Making Changes

### Code Style

We use automated formatters and linters:

```bash
# Format code
black .

# Lint code
ruff check .

# Type check (optional but encouraged)
mypy src/
```

**These run automatically on commit via pre-commit hooks.**

### Commit Messages

Follow [Conventional Commits](https://www.conventionalcommits.org/):

```
feat: add new trust metric for data privacy
fix: correct calculation in finance metric
docs: update architecture documentation
test: add tests for scenario loader
chore: update dependencies
```

### Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_scenarios.py

# Run tests matching pattern
pytest -k "test_scenario"
```

**All tests must pass before submitting PR.**

### Adding Tests

- Place tests in `tests/` directory
- Name test files `test_*.py`
- Use descriptive test function names
- Aim for >80% code coverage

Example:
```python
def test_scenario_loader_validates_tier():
    """Test that scenario loader rejects invalid tiers."""
    from fba_bench_core.scenarios import load_scenario
    import pytest
    
    with pytest.raises(ValueError, match="Invalid tier"):
        load_scenario(tier=5)
```

## Types of Contributions

### ğŸ› Bug Reports

Use the issue template and include:
- Clear description
- Steps to reproduce
- Expected vs actual behavior
- Environment details (OS, Python version)
- Relevant logs or screenshots

### âœ¨ Feature Requests

Describe:
- The problem you're solving
- Proposed solution
- Alternative approaches considered
- Acceptance criteria

### ğŸ“ Documentation

- Fix typos or clarify existing docs
- Add examples or tutorials
- Improve API documentation
- Translate documentation

### ğŸ”§ Code Contributions

#### Adding a New Scenario

1. Create YAML file: `src/fba_bench_core/scenarios/tier_X_name.yaml`
2. Follow scenario schema (see `schemas/scenario_schema.json`)
3. Add tests in `tests/test_scenarios.py`
4. Generate golden master
5. Update documentation

#### Adding a New Metric

1. Create module: `src/fba_bench_core/metrics/your_metric.py`
2. Implement `BaseMetric` interface
3. Add to `__init__.py`
4. Write comprehensive tests
5. Document calculation method

#### Adding a Baseline Agent

1. Create module: `src/fba_bench_core/baseline_bots/your_bot.py`
2. Implement `BaseAgent` interface
3. Benchmark against existing agents
4. Add tests and documentation

## Pull Request Process

### 1. Ensure Quality

Before submitting:
- [ ] All tests pass: `pytest`
- [ ] Code is formatted: `black .`
- [ ] No linting errors: `ruff check .`
- [ ] Documentation updated
- [ ] CHANGELOG.md updated (if applicable)

### 2. Create Pull Request

- Use descriptive title following conventional commits
- Fill out PR template completely
- Link related issues
- Request review from maintainers

### 3. Review Process

- Maintainers will review within 1 week
- Address feedback promptly
- Keep discussion constructive
- Be patient and respectful

### 4. Merging

- Requires approval from 1+ maintainer
- All CI checks must pass
- Squash commits when merging
- Delete branch after merge

## Development Tips

### Running Validation Locally

```bash
# Validate all data files
python scripts/validate_all.py

# Validate scenarios only
python scripts/validate_scenarios.py

# Check for secrets
detect-secrets scan
```

### Building Documentation

```bash
# Install docs dependencies
pip install -e ".[docs]"

# Build documentation
cd docs
make html
```

### Debugging Tests

```bash
# Run with verbose output
pytest -v

# Stop on first failure
pytest -x

# Run specific test with print statements
pytest -s tests/test_scenarios.py::test_load_scenario
```

## Questions?

- **General questions**: Open a GitHub Discussion
- **Bug reports**: Open an issue
- **Security issues**: Email security@fba-bench.ai (DO NOT open public issue)

## License

By contributing, you agree that your contributions will be licensed under the same license as the project (see LICENSE file).

## Recognition

Contributors will be acknowledged in:
- README.md contributors section
- CITATION.cff file
- Release notes

Thank you for contributing! ğŸ‰
```

---

### 10. Create Example Scenarios and Baseline Agents

#### Task 10.1: Create Example Tier 0 Scenario

**`src/fba_bench_core/scenarios/tier_0_baseline.yaml`:**
```yaml
id: tier_0_simple_task
name: "Simple Task Automation"
tier: 0
version: "1.0.0"
description: |
  Basic task automation scenario with minimal constraints.
  Agent must complete a simple data processing task.

metadata:
  created: "2025-01-15T00:00:00Z"
  updated: "2025-01-15T00:00:00Z"
  author: "FBA-Bench Team"

inputs:
  data_file: "sample_data.csv"
  target_format: "json"
  output_file: "processed_data.json"

constraints:
  time_limit: 300  # 5 minutes
  max_api_calls: 50
  allowed_operations:
    - read
    - transform
    - write

evaluation:
  metrics:
    - operations
    - trust
  thresholds:
    pass: 60.0
    excellent: 85.0
  
  success_criteria:
    - name: "Correctness"
      weight: 0.6
      description: "Output matches expected format and content"
    - name: "Efficiency"
      weight: 0.2
      description: "Completes within time and resource limits"
    - name: "Data Handling"
      weight: 0.2
      description: "Properly handles data without leakage"

expected_output:
  format: "json"
  schema:
    type: "array"
    items:
      type: "object"
  min_records: 100
```

#### Task 10.2: Create Simple Baseline Agent

**`src/fba_bench_core/baseline_bots/simple_bot.py`:**
```python
"""Simple rule-based baseline agent."""

from typing import Dict, Any
import time
import json
from ..agents import BaseAgent

class SimpleBot(BaseAgent):
    """
    Simple rule-based agent that follows basic heuristics.
    
    This agent provides a baseline for comparison and demonstrates
    the minimum viable implementation of the agent interface.
    """
    
    def __init__(self, name: str = "SimpleBot"):
        self.name = name
        self.actions_log = []
        self.start_time = None
    
    def execute(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute scenario using simple rule-based logic.
        
        Args:
            scenario: Scenario configuration dictionary
        
        Returns:
            Execution results including actions taken and metrics
        """
        self.start_time = time.time()
        self.actions_log = []
        
        # Extract scenario parameters
        tier = scenario.get("tier", 0)
        constraints = scenario.get("constraints", {})
        
        # Simulate execution
        self._log_action("initialize", {"scenario_id": scenario.get("id")})
        
        # Simple decision logic
        if tier == 0:
            result = self._execute_tier_0(scenario)
        elif tier == 1:
            result = self._execute_tier_1(scenario)
        else:
            result = self._execute_tier_2(scenario)
        
        execution_time = time.time() - self.start_time
        
        return {
            "agent": self.name,
            "scenario_id": scenario.get("id"),
            "success": result.get("success", False),
            "actions": self.actions_log,
            "execution_time": execution_time,
            "result": result
        }
    
    def _execute_tier_0(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Execute tier 0 (baseline) scenario."""
        self._log_action("read_input", {"file": scenario.get("inputs", {}).get("data_file")})
        time.sleep(0.1)  # Simulate processing
        
        self._log_action("transform_data", {"format": "json"})
        time.sleep(0.1)
        
        self._log_action("write_output", {"file": scenario.get("inputs", {}).get("output_file")})
        
        return {"success": True, "records_processed": 100}
    
    def _execute_tier_1(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Execute tier 1 (moderate) scenario."""
        # More complex logic for tier 1
        self._log_action("analyze_constraints", scenario.get("constraints", {}))
        self._log_action("plan_execution", {})
        
        return self._execute_tier_0(scenario)
    
    def _execute_tier_2(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Execute tier 2 (advanced) scenario."""
        # Complex multi-step logic for tier 2
        self._log_action("initialize_coordination", {})
        return self._execute_tier_1(scenario)
    
    def _log_action(self, action_type: str, details: Dict[str, Any]):
        """Log an action taken by the agent."""
        self.actions_log.append({
            "timestamp": time.time() - (self.start_time or time.time()),
            "action": action_type,
            "details": details
        })
    
    def reset(self) -> None:
        """Reset agent state."""
        self.actions_log = []
        self.start_time = None
```

#### Task 10.3: Create Basic Metric Implementation

**`src/fba_bench_core/metrics/base.py`:**
```python
"""Base metric interfaces and utilities."""

from abc import ABC, abstractmethod
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class MetricResult:
    """Result of a metric calculation."""
    score: float  # 0-100
    details: Dict[str, Any]
    passed: bool
    message: str

class BaseMetric(ABC):
    """Base class for all metrics."""
    
    def __init__(self, name: str, weight: float = 1.0):
        self.name = name
        self.weight = weight
    
    @abstractmethod
    def calculate(self, agent_output: Dict[str, Any], scenario: Dict[str, Any]) -> MetricResult:
        """
        Calculate metric score from agent output.
        
        Args:
            agent_output: Results from agent execution
            scenario: Original scenario configuration
        
        Returns:
            MetricResult with score and details
        """
        pass
    
    def normalize_score(self, raw_score: float, min_val: float = 0, max_val: float = 100) -> float:
        """Normalize score to 0-100 range."""
        if max_val == min_val:
            return 50.0
        normalized = ((raw_score - min_val) / (max_val - min_val)) * 100
        return max(0.0, min(100.0, normalized))

class MetricSuite:
    """Collection of metrics for evaluation."""
    
    def __init__(self, metrics: List[BaseMetric]):
        self.metrics = metrics
    
    def evaluate(self, agent_output: Dict[str, Any], scenario: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate agent output using all metrics in suite.
        
        Returns:
            Dictionary with individual metric scores and overall score
        """
        results = {}
        total_weight = sum(m.weight for m in self.metrics)
        weighted_sum = 0.0
        
        for metric in self.metrics:
            result = metric.calculate(agent_output, scenario)
            results[metric.name] = {
                "score": result.score,
                "passed": result.passed,
                "message": result.message,
                "details": result.details
            }
            weighted_sum += result.score * metric.weight
        
        overall_score = weighted_sum / total_weight if total_weight > 0 else 0.0
        
        return {
            "overall_score": overall_score,
            "metrics": results,
            "passed": all(r["passed"] for r in results.values())
        }
```

**`src/fba_bench_core/metrics/operations.py`:**
```python
"""Operations efficiency metrics."""

from typing import Dict, Any
from .base import BaseMetric, MetricResult

class OperationsMetric(BaseMetric):
    """Measures operational efficiency."""
    
    def __init__(self):
        super().__init__("operations", weight=1.0)
    
    def calculate(self, agent_output: Dict[str, Any], scenario: Dict[str, Any]) -> MetricResult:
        """Calculate operations efficiency score."""
        constraints = scenario.get("constraints", {})
        time_limit = constraints.get("time_limit", float('inf'))
        max_api_calls = constraints.get("max_api_calls", float('inf'))
        
        execution_time = agent_output.get("execution_time", 0)
        actions_count = len(agent_output.get("actions", []))
        
        # Calculate time efficiency (0-50 points)
        time_score = 0.0
        if execution_time <= time_limit:
            time_efficiency = 1.0 - (execution_time / time_limit)
            time_score = time_efficiency * 50
        
        # Calculate resource efficiency (0-50 points)
        resource_score = 0.0
        if actions_count <= max_api_calls:
            resource_efficiency = 1.0 - (actions_count / max_api_calls)
            resource_score = resource_efficiency * 50
        
        total_score = time_score + resource_score
        passed = total_score >= 60.0
        
        return MetricResult(
            score=total_score,
            passed=passed,
            message=f"Operations score: {total_score:.1f}/100",
            details={
                "execution_time": execution_time,
                "time_limit": time_limit,
                "actions_count": actions_count,
                "max_api_calls": max_api_calls,
                "time_score": time_score,
                "resource_score": resource_score
            }
        )
```

---

## Developer Experience (P2)

#### Status and Outputs

Status: Completed.

Key outputs:
- Containerization and dev environment:
  - [Dockerfile](Dockerfile), [docker-compose.yml](docker-compose.yml), [.devcontainer/devcontainer.json](.devcontainer/devcontainer.json)
- Quickstart documentation:
  - [docs/quickstart.md](docs/quickstart.md)
- Package metadata and scenario loader shim:
  - [pyproject.toml](pyproject.toml)
  - [src/fba_bench_core/scenarios/__init__.py](src/fba_bench_core/scenarios/__init__.py)
- README requirements:
  - [README.md](README.md) updated with Python version and install steps

### 11. Add Docker Support

**`Dockerfile`:**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy dependency files
COPY pyproject.toml ./
COPY README.md ./

# Install Python dependencies
RUN pip install --no-cache-dir -e ".[dev]"

# Copy source code
COPY . .

# Run validation on build
RUN python scripts/validate_all.py

CMD ["pytest"]
```
_Implemented in repo at [Dockerfile](Dockerfile)._

**`docker-compose.yml`:**
```yaml
version: '3.8'

services:
  fba-bench:
    build: .
    volumes:
      - .:/app
      - /app/.venv  # Don't mount venv
    environment:
      - PYTHONUNBUFFERED=1
    command: pytest -v
  
  dev:
    build: .
    volumes:
      - .:/app
      - /app/.venv
    environment:
      - PYTHONUNBUFFERED=1
    command: bash
    stdin_open: true
    tty: true
```
_Implemented in repo at [docker-compose.yml](docker-compose.yml)._

**`.devcontainer/devcontainer.json`:**
```json
{
  "name": "FBA-Bench Core Development",
  "dockerComposeFile": "../docker-compose.yml",
  "service": "dev",
  "workspaceFolder": "/app",
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance",
        "charliermarsh.ruff",
        "ms-python.black-formatter",
        "redhat.vscode-yaml",
        "eamodio.gitlens"
      ],
      "settings": {
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "python.linting.enabled": true,
        "python.linting.ruffEnabled": true,
        "python.formatting.provider": "black",
        "editor.formatOnSave": true,
        "editor.codeActionsOnSave": {
          "source.organizeImports": true
        }
      }
    }
  },
  "postCreateCommand": "pip install -e '.[dev]' && pre-commit install"
}
```

---

### 12. Create Quickstart Tutorial

**`docs/quickstart.md`:**
```markdown
# Quickstart Guide

Get started with FBA-Bench Core in 10 minutes!

## Installation

```bash
# Clone repository
git clone https://github.com/Bender1011001/FBA-Bench-core.git
cd FBA-Bench-core

# Create virtual environment
python3.9 -m venv .venv
source .venv/bin/activate  # or .\.venv\Scripts\Activate.ps1 on Windows

# Install
pip install -e .
```

## Run Your First Benchmark

### 1. Load a Scenario

```python
from fba_bench_core.scenarios import load_scenario

# Load tier 0 baseline scenario
scenario = load_scenario(tier=0)
print(f"Loaded: {          }
          }
          
          try:
              with open('site/data/leaderboard.json') as f:
                  data = json.load(f)
              validate(instance=data, schema=schema)
              print("âœ… Leaderboard JSON is valid")
          except ValidationError as e:
              print(f"âŒ Validation error: {e}")
              sys.exit(1)
          PY

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Bandit security scan
        run: |
          pip install bandit
          bandit -r src/ -f json -o bandit-report.json || true
      
      - name: Check for secrets
        run: |
          pip install detect-secrets
          detect-secrets scan --baseline .secrets.baseline
```

**`.github/workflows/release.yml`:**
```yaml
name: Release

on:
  push:
    tags:
      - 'v*'

jobs:
  release:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          draft: false
          prerelease: false
          generate_release_notes: true
          files: |
            site/assets/press/press-kit.zip
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

#### Task 4.2: Create Validation Scripts

**`scripts/validate_scenarios.py`:**
```python
#!/usr/bin/env python3
"""Validate scenario YAML files against schema."""

import sys
from pathlib import Path
import yaml
from jsonschema import validate, ValidationError

SCENARIO_SCHEMA = {
    "type": "object",
    "required": ["id", "name", "tier", "inputs", "constraints", "evaluation"],
    "properties": {
        "id": {"type": "string"},
        "name": {"type": "string"},
        "tier": {"type": "integer", "minimum": 0, "maximum": 2},
        "description": {"type": "string"},
        "inputs": {"type": "object"},
        "constraints": {"type": "object"},
        "evaluation": {
            "type": "object",
            "required": ["metrics"],
            "properties": {
                "metrics": {"type": "array"}
            }
        }
    }
}

def validate_scenario(filepath: Path) -> bool:
    """Validate a single scenario file."""
    try:
        with open(filepath) as f:
            data = yaml.safe_load(f)
        
        validate(instance=data, schema=SCENARIO_SCHEMA)
        print(f"âœ… {filepath}")
        return True
    
    except ValidationError as e:
        print(f"âŒ {filepath}: {e.message}")
        return False
    except Exception as e:
        print(f"âŒ {filepath}: {e}")
        return False

def main():
    scenario_dir = Path("src/scenarios")
    if not scenario_dir.exists():
        print(f"âš ï¸  {scenario_dir} not found")
        return 0
    
    scenarios = list(scenario_dir.glob("**/*.yaml")) + list(scenario_dir.glob("**/*.yml"))
    
    if not scenarios:
        print("âš ï¸  No scenario files found")
        return 0
    
    results = [validate_scenario(s) for s in scenarios]
    
    passed = sum(results)
    total = len(results)
    
    print(f"\nğŸ“Š Results: {passed}/{total} passed")
    return 0 if all(results) else 1

if __name__ == "__main__":
    sys.exit(main())
```
_Implemented in repo at [scripts/validate_scenarios.py](scripts/validate_scenarios.py)._

**Make executable:**
```bash
chmod +x scripts/validate_scenarios.py
```

---

### 5. Fix Python Package Structure

#### Task 5.1: Create Proper Package Configuration

**`pyproject.toml`:**
```toml
[build-system]
requires = ["setuptools>=68.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "fba-bench-core"
version = "0.1.0"
description = "Benchmark suite for AI agents in financial business automation"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}  # Update with chosen license
authors = [
    {name = "Your Name", email = "contact@fba-bench.ai"}
]
keywords = ["benchmark", "ai", "agents", "automation"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",  # Update
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "pyyaml>=6.0",
    "jsonschema>=4.0",
    "numpy>=1.24",
    "pandas>=2.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "ruff>=0.1.0",
    "black>=23.0",
    "mypy>=1.0",
    "pre-commit>=3.0",
]
docs = [
    "sphinx>=7.0",
    "sphinx-rtd-theme>=1.3",
]

[project.urls]
Homepage = "https://github.com/Bender1011001/FBA-Bench-core"
Documentation = "https://fba-bench-core.readthedocs.io"  # If applicable
Repository = "https://github.com/Bender1011001/FBA-Bench-core"
Issues = "https://github.com/Bender1011001/FBA-Bench-core/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["src*", "metrics*", "agents*", "baseline_bots*", "fba_bench_core*"]

[tool.ruff]
line-length = 100
target-version = "py39"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
    "UP",  # pyupgrade
]
ignore = [
    "E501",  # line too long (handled by black)
    "B008",  # do not perform function calls in argument defaults
]

[tool.black]
line-length = 100
target-version = ['py39', 'py310', 'py311', 'py312']

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false  # Start permissive, tighten later
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
addopts = "-v --tb=short --strict-markers"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
]
```

#### Task 5.2: Normalize Directory Structure

**Recommended structure:**
```
FBA-Bench-core/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ release.yml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ paper/
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fba_bench_core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ scenarios/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tier_0_baseline.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ tier_1_moderate.yaml
â”‚   â”‚   â”‚   â””â”€â”€ tier_2_advanced.yaml
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ trust.py
â”‚   â”‚   â”‚   â”œâ”€â”€ operations.py
â”‚   â”‚   â”‚   â””â”€â”€ finance.py
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ base_agent.py
â”‚   â”‚   â””â”€â”€ baseline_bots/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ simple_bot.py
â”‚   â”‚       â””â”€â”€ rule_based_bot.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scenarios.py
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â””â”€â”€ test_agents.py
â”œâ”€â”€ golden_masters/
â”‚   â”œâ”€â”€ tier_0/
â”‚   â”œâ”€â”€ tier_1/
â”‚   â””â”€â”€ tier_2/
â”œâ”€â”€ site/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ leaderboard.html
â”‚   â”œâ”€â”€ press.html
â”‚   â”œâ”€â”€ research.html
â”‚   â”œâ”€â”€ config.js
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ leaderboard.json
â”‚   â””â”€â”€ assets/
â”‚       â””â”€â”€ press/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate_scenarios.py
â”‚   â”œâ”€â”€ add_license_header.py
â”‚   â””â”€â”€ inject_config.sh
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ .secrets.baseline
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ CITATION.cff
â””â”€â”€ README.md
```

**Remove the `repos/fba-bench-core/` nesting:**
```bash
# If this exists, flatten it
if [ -d "repos/fba-bench-core" ]; then
  mv repos/fba-bench-core/* .
  rm -rf repos/
fi
```

#### Task 5.3: Create Package Init Files

**`src/fba_bench_core/__init__.py`:**
```python
"""
FBA-Bench Core: Benchmark suite for AI agents in financial business automation.

Copyright (c) 2025 [Your Organization]
Licensed under the [License Name].
"""

__version__ = "0.1.0"
__author__ = "Your Name"
__email__ = "contact@fba-bench.ai"

from .scenarios import load_scenario
from .metrics import MetricSuite
from .agents import BaseAgent

__all__ = [
    "load_scenario",
    "MetricSuite",
    "BaseAgent",
    "__version__",
]
```

**`src/fba_bench_core/scenarios/__init__.py`:**
```python
"""Scenario loading and management."""

from pathlib import Path
import yaml
from typing import Dict, Any

def load_scenario(tier: int) -> Dict[str, Any]:
    """Load a scenario by tier number.
    
    Args:
        tier: Scenario tier (0, 1, or 2)
    
    Returns:
        Scenario configuration dictionary
    
    Raises:
        ValueError: If tier is invalid
        FileNotFoundError: If scenario file doesn't exist
    """
    if tier not in (0, 1, 2):
        raise ValueError(f"Invalid tier: {tier}. Must be 0, 1, or 2.")
    
    scenario_map = {
        0: "tier_0_baseline.yaml",
        1: "tier_1_moderate.yaml",
        2: "tier_2_advanced.yaml",
    }
    
    scenario_path = Path(__file__).parent / scenario_map[tier]
    
    if not scenario_path.exists():
        raise FileNotFoundError(f"Scenario file not found: {scenario_path}")
    
    with open(scenario_path) as f:
        return yaml.safe_load(f)

__all__ = ["load_scenario"]
```

---

### 6. Add Python Version Requirements

**Update README.md:**
```markdown
## Requirements

- **Python**: 3.9 or higher (tested on 3.9, 3.10, 3.11, 3.12)
- **Operating System**: Linux, macOS, or Windows
- **Dependencies**: See `pyproject.toml` for full list

## Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Bender1011001/FBA-Bench-core.git
   cd FBA-Bench-core
   ```

2. **Create a virtual environment:**
   ```bash
   python3.9 -m venv .venv
   
   # Activate on Linux/macOS:
   source .venv/bin/activate
   
   # Activate on Windows (PowerShell):
   .\.venv\Scripts\Activate.ps1
   
   # Activate on Windows (cmd):
   .\.venv\Scripts\activate.bat
   ```

3. **Install the package:**
   ```bash
   pip install -e .
   
   # Or with development dependencies:
   pip install -e ".[dev]"
   ```

4. **Verify installation:**
   ```bash
   python -c "import fba_bench_core; print(fba_bench_core.__version__)"
   ```
```

---

## Production Hardening (P1)

#### Status and Outputs

Status: Completed.

Key outputs:
- Automated asset pipeline:
  - [scripts/build_assets.py](scripts/build_assets.py)
  - Integrated in CI: see assets step in [.github/workflows/ci.yml](.github/workflows/ci.yml)
- Schema validation hardening:
  - [schemas/scenario_schema.json](schemas/scenario_schema.json), [schemas/leaderboard_schema.json](schemas/leaderboard_schema.json)
  - CI validators: [scripts/validate_all.py](scripts/validate_all.py), [scripts/validate_scenarios.py](scripts/validate_scenarios.py), wired in [.github/workflows/ci.yml](.github/workflows/ci.yml)

**Timeline**: 3-4 weeks after P0 completion

### 7. Implement Automated Asset Pipeline

#### Task 7.1: Create Asset Build Script

**`scripts/build_assets.py`:**
Implemented at [scripts/build_assets.py](scripts/build_assets.py)
```python
#!/usr/bin/env python3
"""Build and optimize assets for distribution."""

import shutil
import subprocess
from pathlib import Path
from zipfile import ZipFile, ZIP_DEFLATED

def optimize_images():
    """Optimize PNG and SVG files."""
    assets_dir = Path("site/assets/press")
    
    # Optimize PNGs with optipng (if available)
    png_files = list(assets_dir.glob("**/*.png"))
    if png_files and shutil.which("optipng"):
        for png in png_files:
            subprocess.run(["optipng", "-o7", str(png)], check=False)
            print(f"âœ… Optimized {png}")
    
    # Optimize SVGs with svgo (if available)
    svg_files = list(assets_dir.glob("**/*.svg"))
    if svg_files and shutil.which("svgo"):
        for svg in svg_files:
            subprocess.run(["svgo", str(svg)], check=False)
            print(f"âœ… Optimized {svg}")

def create_press_kit():
    """Create press kit ZIP file."""
    assets_dir = Path("site/assets/press")
    output_zip = assets_dir / "press-kit.zip"
    
    # Remove old ZIP if exists
    if output_zip.exists():
        output_zip.unlink()
    
    # Create new ZIP
    with ZipFile(output_zip, 'w', ZIP_DEFLATED) as zipf:
        # Add all files from press directory
        for file in assets_dir.rglob("*"):
            if file.is_file() and file.name != "press-kit.zip":
                arcname = file.relative_to(assets_dir)
                zipf.write(file, arcname)
                print(f"ğŸ“¦ Added {arcname}")
    
    size_mb = output_zip.stat().st_size / (1024 * 1024)
    print(f"\nâœ… Created press-kit.zip ({size_mb:.2f} MB)")

def validate_assets():
    """Validate that all required assets exist."""
    required_assets = [
        "site/assets/press/logo-light.svg",
        "site/assets/press/logo-dark.svg",
        "site/assets/press/logo.png",
        "site/assets/press/og-image.png",
    ]
    
    missing = []
    for asset in required_assets:
        if not Path(asset).exists():
            missing.append(asset)
    
    if missing:
        print("âŒ Missing required assets:")
        for asset in missing:
            print(f"   - {asset}")
        return False
    
    print("âœ… All required assets present")
    return True

def main():
    print("ğŸ”¨ Building assets...\n")
    
    if not validate_assets():
        print("\nâš ï¸  Please add missing assets before building")
        return 1
    
    optimize_images()
    create_press_kit()
    
    print("\nâœ¨ Asset build complete!")
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())
```

**Make executable and add to CI:**
```bash
chmod +x scripts/build_assets.py

# Add to .github/workflows/ci.yml:
# - name: Build assets
#   run: python scripts/build_assets.py
```

---

### 8. Schema Validation for Data Files

#### Task 8.1: Create Comprehensive Schemas

**`schemas/scenario_schema.json`:**
Implemented at [schemas/scenario_schema.json](schemas/scenario_schema.json)
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "FBA-Bench Scenario",
  "type": "object",
  "required": ["id", "name", "tier", "inputs", "constraints", "evaluation"],
  "properties": {
    "id": {
      "type": "string",
      "pattern": "^[a-z0-9_]+$",
      "description": "Unique scenario identifier"
    },
    "name": {
      "type": "string",
      "minLength": 1,
      "maxLength": 100
    },
    "tier": {
      "type": "integer",
      "minimum": 0,
      "maximum": 2,
      "description": "Complexity tier: 0 (baseline), 1 (moderate), 2 (advanced)"
    },
    "description": {
      "type": "string",
      "maxLength": 500
    },
    "inputs": {
      "type": "object",
      "description": "Input parameters for the scenario"
    },
    "constraints": {
      "type": "object",
      "properties": {
        "time_limit": {
          "type": "number",
          "minimum": 0,
          "description": "Maximum execution time in seconds"
        },
        "budget_limit": {
          "type": "number",
          "minimum": 0,
          "description": "Budget constraint"
        }
      
---
## Next Actions

1) Provide the real ARXIV_ID and replace the placeholder token ARXIV_ID across:
   - [CITATION.cff](CITATION.cff)
   - [README.md](README.md)
   - [site/research.html](site/research.html)
   - [docs/paper/README.md](docs/paper/README.md)

Example commands:
- POSIX shell:
```bash
git grep -l 'ARXIV_ID' | xargs -I{} sed -i'' -e 's/ARXIV_ID/2501.01234/g' {}
```

- PowerShell (Windows):
```powershell
git grep -l 'ARXIV_ID' | % { (Get-Content $_) -replace 'ARXIV_ID','2501.01234' | Set-Content $_ }
```

2) Re-run CI and validators to confirm all status is green:
- GitHub Actions workflow: [.github/workflows/ci.yml](.github/workflows/ci.yml)
- Local validation:
```bash
python scripts/validate_all.py
python scripts/validate_scenarios.py
```

No remaining placeholder tokens should exist in this guide. License choice implemented: MIT (see [LICENSE](LICENSE) and [pyproject.toml](pyproject.toml)).
---